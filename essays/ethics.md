Ethics in any field is not a black and white matter. In fields that have existed for millenniums such as philosophy there still remain many unanswered ethical questions.  For a relatively new field such as computer science, and within it, the burgeoning field of artificial intelligence these urgent ethical questions show unresolved issues that must be answered before certain applications can move forward. 

Within software engineering, ethics is a very difficult topic to analyze. While there are issues within software engineering that are easy to see as ethical or unethical, this is usually not the case. For example selling data to a third party, or conversely creating open source software, are unambiguous in their ethically, there exists many subjects that are not as clear-cut. On a personal level I see ethics in the field of software engineering as creating software that has the intention of doing good or bad, with particular emphasis on intention. The reason for this emphasis is that unlike many other fields software is much more prone to unintended outcomes from the innate randomness that is incorporated into every aspect of the field. 

In greater detail the case of automated vehicles can be analyzed. The major ethical controversy that exists for automated vehicles is analogous to that of the utilitarian trolley dilemma. If there were a group of pedestrians standing on the road, is it more ethical for the car to swerve and kill the driver? To stay its course and hit the crowd? What if instead of a crowd there is a single person on the road? What if the passengers are not just one driver, but instead a family? Utilitarianism argues that choosing the case where the most life is preserved is the correct option, but is this necessarily correct? There are  many undetermined and unique factors in every situation, how do we, as software engineers create “ethical” software to deal with this type of situation?

In my opinion the “correct” choice is that the person operating the vehicle should be the one to suffer the consequences independent of who is operating the vehicle or how many people are at risk.  In both readings this leads to the catch-22 of people agreeing that the automated vehicles are safer, but no one wanting to purchase automated vehicles if they are the ones at risk. However, this reduces the problem into one not of ethical implications, but one of commercial. When the question is posed as the dilemma: why should a company program automated cars in a utilitarian sense in order to preserve lives, at the cost of sales. The problem can then be reduced to asking if profits are more important than those of safety.

As I view ethics in software engineering it does seem that the ethical choice is the utilitarian one. However, when it comes to ethics nothing is unambiguous. In order to make ethical software, and to be an ethical software engineer, every possibility must be analyzed. Even from my previous argument I do not consider many factors. What if the person in the car is the President? A child? What if the crowd of people consists of Lex Luthor, Geoffrey Baratheon, and Voldemort? Ethics within any field is incredibly important. As we progress within software engineering it is our responsibility to analyze what we are doing and ask if our decisions are ethical. 
